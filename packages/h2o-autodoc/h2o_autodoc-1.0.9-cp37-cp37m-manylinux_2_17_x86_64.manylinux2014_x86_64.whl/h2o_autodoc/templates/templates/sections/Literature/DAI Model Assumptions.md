#### Data types supported

- Tabular structured data: rows are observations, columns are fields, features, or variables.

- IID (independent and identically distributed) data.

- Numeric, categorical and textual fields.

- Data with missing values.

- Time-series data with a single time-series (i.e., time flows across the entire dataset, not per block of data).

- Grouped time-series (e.g., sales per store per department per week, all in one file, with three columns one for store, department, and week).

- Time-series problems with a gap between training and testing (i.e., the time to deploy), and a known forecast horizon (after which model has to be retrained).

#### Problem types supported

- Regression: a continuous target variable (e.g., income, house price, loss prediction, time-series forecasting).

- Binary classification: a binary target (e.g., 0/1 or “No”/”Yes”, for use cases like fraud prediction, churn prediction, failure prediction, etc.).

- Multinomial classification: a multinomial target (e.g., 0/1/2/3 or “A”/”B”/”C”/”D” for categorical target variables in uses cases like prediction of membership type, next-action, product recommendation, etc.){% if not experiment.parameters.is_timeseries %}

- Time series

For time series problems, the data is assumed to be homogeneously distributed with respect to time. Each observation (i.e., row or record) has to be uniquely identifiable by a timestamp t and optional time groups columns TGC. This means within one group no timestamp should appear more than once. Driverless AI time series further assumes, that all groups share the same frequency (for example, all groups represent weekly data).

{% endif %}

#### Experiment Details

Driverless AI trains all models based on the training data provided (in this case: {{ train_data.name }} ). It is the assumption of Driverless AI that this dataset is representative of the data that will be seen when scoring. 

For the *{{experiment.description}}* experiment, Driverless AI built {% if final_model._num_ensemble_models > 0 %} a Stacked Ensemble ({{final_model._final_model_string}}).{% else %} {{final_model._final_model_string}}. {% endif %} {% if final_model._num_ensemble_models > 0 %} As a Stacked Ensemble inherits the assumptions of its base learners, the base-learner assumptions, if applicable, are described below. {% else %} The model  assumptions are described below. {% endif %} {% if final_model._glm_in_final %}

Generalized Linear Models require the following assumptions to be satisfied:

- A density function $f=f(y;θ,ϕ)$ distributed as an exponential family and parameterized by $θ$ and $ϕ$, can model the given dependent variable y. This removes the restriction on the distribution of the error and allows for non-homogeneity of the variance with respect to the mean vector.

- The systematic component $η$ is defined linearly as $η=Xß$, where $X$ is the matrix of all observation vectors $x^i$, where  $i=1,2,…,m$ and $m$ is the total number of observations. Where $ß$ can be parameterized by $θ$ and/or $ϕ$.

- There is a link function $g$, such that $g^{-1}(η)=E(y)$, which relates the expected value of the response to the systematic component η. The link function can be any monotonic differentiable function. This relaxes the constraints on the additivity of the covariates, and it allows the response to belong to a restricted range of values depending on the chosen transformation $g$.

{% endif %}

{% if  ('RuleFitModel' in final_model._final_model_string) and  (not final_model._glm_in_final) %}

The RuleFit algorithm is an implementation of a Generalized Linear Model (GLM), which includes features generated by tree-based models, as a result its main assumptions follow those of a standard GLM:

- A density function $f=f(y;θ,ϕ)$ distributed as an exponential family and parameterized by $θ$ and $ϕ$, can model the given dependent variable y. This removes the restriction on the distribution of the error and allows for non-homogeneity of the variance with respect to the mean vector.

- The systematic component $η$ is defined linearly as $η=Xß$, where $X$ is the matrix of all observation vectors $x^i$, where  $i=1,2,…,m$ and $m$ is the total number of observations. Where $ß$ can be parameterized by $θ$ and/or $ϕ$.

- There is a link function $g$, such that $g^{-1}(η)=E(y)$, which relates the expected value of the response to the systematic component $η$. The link function can be any monotonic differentiable function. This relaxes the constraints on the additivity of the covariates, and it allows the response to belong to a restricted range of values depending on the chosen transformation $g$.

{% endif %}

{% if  ('RuleFitModel' in final_model._final_model_string) and  final_model._glm_in_final %}

The RuleFit algorithm is an implementation of a Generalized Linear Model (GLM), which includes features generated by tree-based models, as a result its main assumptions follow those of a standard GLM.

{% endif %}

{% if ('XGBoostGBMModel' in final_model._final_model_string) or ('LightGBMModel' in final_model._final_model_string) or ('XGBoostDartModel' in final_model._final_model_string) %}

Gradient-Boosting-based models (XGBoost, LightGBM, etc.) are machine learning algorithms that do not make assumptions about the input dataset. The only expectation is that the data type and problem type fall into the Driverless AI supported categories (specified above).

{% endif %}

{% if  'TensorFlowModel' in final_model._final_model_string %}

The TensorFlow model implements a multi-layer perceptron (feed-forward neural net), which, depending on the internal DAI configurations, could contain additional skip connection (similar to ResNet architecture) and attention mechanisms. The search space for this model consists of number of layers, number of neurons, activation options, number of epochs, batch size, dropout and learning rate. DAI will experiment with deep models (many layers, fewer neurons per layer) and wide models (fewer layers, more neurons per layer).

Skip Connection Reference: K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.

Attention Mechanism Reference: J. Hu, L. Shen, S. Albanie, G. Sun and E. Wu, "Squeeze-and-Excitation Networks," in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2019.2913372.{% endif %}

{% if  'FTRLModel' in final_model._final_model_string %}

Warning FTRL models are not supported in this automatic report.

{% endif %}

{% if (params._params.is_classification == False) and (model_tuning._target_tuning == None) %}

Based on the experiment's accuracy setting, Driverless AI did not apply any target transformations. As a consequence, model performance may suffer, as Driverless AI is optimized for targets with a Gaussian distribution. 

{% elif  (params._params.is_classification == False) and (model_tuning._target_tuning != None) %}

For this regression problem, Driverless AI performed target tuning to determine the best way to represent the target column (for example: checking if the log transform of the target could generate better results). For the *{{experiment.description}}* experiment, specifically, Driverless AI tested the following target transformation(s):

{% for item in model_tuning._target_tuning.transformers_used %}

- {{ item }}

{% endfor %}

{% endif %}


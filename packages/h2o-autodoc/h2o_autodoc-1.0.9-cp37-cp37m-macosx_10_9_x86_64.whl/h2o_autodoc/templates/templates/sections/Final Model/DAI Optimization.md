**Optimization**

Driverless AI uses random grid search and a genetic algorithm to optimize the {{ experiment.score_f_name }} across models.  {% if experiment_overview._iteration_info.stopping_reason == "Early Stopping" %} It is the assumption of Driverless AI, that after {{experiment_overview._iteration_info.actual_num_iterations}} iterations, the software was able to find the optimal set of features due to detected convergence. {% endif %} Driverless AI trained {{model_tuning._tuning_algos}} during the experiment.  Each algorithm has a different optimization technique. 

{% if (model_tuning._tuning_algos.lower().replace(',','').split().count('xgboost') == 2) or  ( (model_tuning._tuning_algos.lower().replace(',','').split().count('xgboost') == 1) and ("dart" not in model_tuning._tuning_algos)) %}

- The **XGBoost gbtree** algorithm employs the Second-Order Stochastic Gradient Descent optimization algorithm.  The XGBoost gbtree algorithm creates a series of decision trees using an additive strategy where the final model function is the summation of the leaf scores from the decision trees.  When creating a single decision tree, the algorithm uses a greedy search procedure to evaluate the optimal split column and split point.  The optimization goal of the greedy search procedure is the gradient of the objective function.  See the table above for the objective function used. {% endif %} {% if "dart" in model_tuning._tuning_algos %}

- The **XGBoost dart** algorithm employs the Second-Order Stochastic Gradient Descent optimization algorithm.  The XGBoost dart algorithm creates a series of decision trees using an additive strategy where the final model function is the summation of the leaf scores from the decision trees. To select the next optimal tree structure, the algorithm computes the gradient of its objective function on a random subset of the previously built trees. To account for dropped trees, dart then applies a normalization factor to the new tree. When creating a single decision tree, the algorithm uses a greedy search procedure to evaluate the optimal split column and split point. The optimization goal of the greedy search procedure is the gradient of the objective function. See the table above for the objective function used.{% endif %}{% if "linear" in model_tuning._tuning_algos %}

- The **XGBoost gblinear** algorithm employs Coordinate Gradient Descent optimization algorithm.  The XGBoost gblinear algorithm evaluates a series of alpha and lambda parameters to determine the optimal parameters of the linear model.  The optimization goal of the greedy search procedure is the gradient of the objective function.  See the table above for the objective function used.{% endif %} {% if "light gbm" in model_tuning._tuning_algos %}

- The **Light GBM** algorithm employs the GOSS (Gradient-based One-Side Sampling) optimization algorithm.  The Light GBM algorithm creates a series of decision trees using an additive strategy where the final model function is the summation of the leaf scores from the decision trees.  Each individual decision tree is created to minimize the gradient of the objective function. When creating a single decision tree, the algorithm samples the data to observations with a large gradient.  The assumption is that observations with small gradient are already well-trained.  The approach reduces the size of the data evaluated to determine the optimal split.  See the table above for the objective function used.{% endif %} {% if "rulefit" in model_tuning._tuning_algos %}

- The **RuleFit** algorithm is a combination of a Gradient Boosting Tree Model and Generalized Linear Model.  The approach is to create rules by training a GBM model (traverse the trees to get a series of rules) and then train a linear model for the final prediction using the rules as predictors.  The Gradient Boosting Tree Model is a LightGBM model that uses the GOSS (Gradient-based One-Side Sampling) optimization algorithm. The Light GBM algorithm creates a series of decision trees using an additive strategy where the final model function is the summation of the leaf scores from the decision trees.  Each individual decision tree is created to minimize the gradient of the objective function. When creating a single decision tree, the algorithm samples the data to observations with a large gradient.  The assumption is that observations with small gradient are already well-trained.  The approach reduces the size of the data evaluated to determine the optimal split.  The Generalized Linear Model is from the Scikit-learn package.  It employs the Stochastic Gradient Descent solver and uses L1 regularization.{% endif %}{% if "tensorflow" in model_tuning._tuning_algos %}

- The **TensorFlow** multi-layer perceptron algorithm employs the Adam (adaptive moment estimation) optimization algorithm. Adam is an optimizer that sets the learning rate for each parameter independently. Whereas RMSProp sets the parameter learning rates based on the average gradient for each parameter, Adam also makes use of the average of the second moments of the gradients (the uncentered variance). The algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.  {% endif %}

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/agents/benchmark_agents/02_SAA_multi_period_agents.ipynb.

# %% auto 0
__all__ = ['SAA_MP_Agent', 'SAA_MP_Policy']

# %% ../../../nbs/agents/benchmark_agents/02_SAA_multi_period_agents.ipynb 4
# General libraries:
import numpy as np
import pulp as pl
from tqdm import tqdm

# Mushroom libraries
from mushroom_rl.core import Agent

# %% ../../../nbs/agents/benchmark_agents/02_SAA_multi_period_agents.ipynb 7
class SAA_MP_Agent(Agent):

    train_directly = True
    train_mode = "direct"

    def __init__(self,
                    mdp_info,
                    mdp,
                    h,
                    cu,
                    l,
                    horizon=10,
                    unit_size=0.01,
                    num_scenarios=3,
                    preprocessors = None,
                    postprocessors = None,
                    agent_name = "SAA_MP",
                    precision=5,
                    ):

        self.name = agent_name

        self.num_scenarios=num_scenarios
        
        policy = SAA_MP_Policy(
            h=h,
            cu=cu,
            l=l,
            horizon=horizon,
            mdp = mdp,
            unit_size=unit_size,
            precision=precision,
            preprocessors=preprocessors,
            postprocessors=postprocessors,
        )

        self.precision = precision

        self.train_directly = True
        self.train_mode = "direct"
        self.skip_val = True

        super().__init__(mdp_info, policy)

    def fit(self, features = None, demand=None):

        self.policy.get_scenarios(demand, self.num_scenarios)


class SAA_MP_Policy():
    def __init__(self,
        h,
        cu,
        l,
        horizon,
        mdp,
        unit_size,
        precision,
        preprocessors,
        postprocessors
    ):
        self.h = h
        self.cu = cu
        self.l = l
        self.horizon=horizon

        if preprocessors is None:
            self.preprocessors = list()
        else:
            self.preoprocessors = (preprocessors)
        if postprocessors is None:
            self.postprocessors = list()
        else:
            self.postprocessors = (postprocessors)
        
        self.scenarios = {0: [x for x in range(self.horizon)]}
        self.weights = {0: 1}

        self.counter=0

    def draw_action(self, input):
        for preprocessor in self.preprocessors:
            input = preprocessor(input)
        
        optimization_results = self.optimize(input)
        action = optimization_results['Order']
        
        return np.array([action])

    def optimize(self, state):

        state = {'inventory': state[0], 'pipeline': state[1:]}
        cu = self.cu
        l = self.l
        h = self.h
        horizon = self.horizon
        scenarios = self.scenarios
        weights = self.weights

        T_ = range(horizon)

        # Create the model
        model = pl.LpProblem("Lost_Sales", pl.LpMinimize)

        # Create the decision variables
        ts = []
        for t in T_:
            for s in scenarios.keys():
                ts.append((t,s))

        q = pl.LpVariable.dicts("q", ts, lowBound=0)
        I = pl.LpVariable.dicts("i", ts, lowBound=0)
        u = pl.LpVariable.dicts("u", ts, lowBound=0)

            # Add the objective function
        model += pl.lpSum([weights[s]*(cu*u[t,s] + h*I[t,s]) for t,s in ts])

            # Add the constraints
        I_start = state['inventory']
        p = state['pipeline']

        for s in scenarios:
            model += q[0,s] == q[0,0]

        for s in scenarios:
            for t in T_:
                if t == 0:
                    if l == 0:
                        model += I[t,s] == q[t-l, s] + I_start  - scenarios[s][t] + u[t,s]
                        model += u[t,s] >= scenarios[s][t] - I_start - q[t-l, s]
                    else:
                        model += I[t,s] == p[t] + I_start - scenarios[s][t] + u[t,s]
                        model += u[t,s] >= scenarios[s][t] - I_start - p[t]
                elif t < l:
                    model += I[t,s] == p[t] + I[t-1,s] - scenarios[s][t] + u[t,s]
                    model += u[t,s] >= scenarios[s][t] - I[t-1,s] - p[t]
                else:
                    model += I[t,s] == q[t-l,s] + I[t-1,s] - scenarios[s][t] + u[t,s]
                    model += u[t,s] >= scenarios[s][t] - I[t-1,s] - q[t-l,s]

        # Solve the optimization problem
        # # model.solve(pl.HiGHS_CMD())
        # model.solve()
        model.solve(pl.GUROBI_CMD())
        # Print the results
        results = {
            "Cost": sum([weights[s] * (cu*u[t,s].varValue + h*I[t,s].varValue) for t,s in ts]),
            "Order": q[0,0].varValue,
            }
        
        self.counter +=1
        print("selected: ", results['Order'], "in iteration:", self.counter)

        return results

    def get_scenarios(self, demand, num_scenarios):

        scenarios = {}
        weights = {}
        total_scenarios = len(demand) - self.horizon

        if num_scenarios >= total_scenarios:
            # Use all available scenarios
            for i in range(total_scenarios):
                scenarios[i] = demand[i:i+self.horizon]
                weights[i] = 1 / total_scenarios
        else:
            # Randomly sample scenarios and assign weights
            sampled_indices = np.random.choice(range(total_scenarios), num_scenarios, replace=False)
            for i, idx in enumerate(sampled_indices):
                scenarios[i] = demand[idx:idx+self.horizon]
                weights[i] = 1 / num_scenarios

        self.scenarios = scenarios
        self.weights = weights

        print(len(self.scenarios), "scenarios selected")

    def reset(*args, **kwargs): 
        pass
